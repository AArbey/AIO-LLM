services:
  anythingllm:
    image: mintplexlabs/anythingllm
    container_name: anythingllm
    ports:
    - "3001:3001"
    cap_add:
      - SYS_ADMIN
    environment:
      - STORAGE_DIR=/app/server/storage
    env_file:
      - .env
    volumes:
      - anythingllm_storage:/app/server/storage
    restart: always


  localai:
    environment:
      - LOCALAI_SINGLE_ACTIVE_BACKEND=true
      - DEBUG=true
      - LOCALAI_WATCHDOG_IDLE=true
      - LOCALAI_API_KEY=${LOCAL_AI_API_KEY}
    image: localai/localai:master-gpu-nvidia-cuda-12
    # For images with python backends, use:
    # image: localai/localai:master-cublas-cuda12-ffmpeg
    command: 
    - ${MODEL_NAME:-gemma-3-4b-it-qat}
    - ${MULTIMODAL_MODEL:-moondream2-20250414}
    - ${IMAGE_MODEL:-sd-1.5-ggml}
    - granite-embedding-107m-multilingual
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 60s
      timeout: 10m
      retries: 120
    ports:
    - 8080:8080
    volumes:
      - ./volumes/models:/models
      - ./volumes/backends:/backends
      - ./volumes/images:/tmp/generated/images
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  dind:
    image: docker:dind
    privileged: true
    environment:
      - DOCKER_TLS_CERTDIR=""
    healthcheck:
      test: ["CMD", "docker", "info"]
      interval: 10s
      timeout: 5s
      retries: 3

  localrecall:
    image: quay.io/mudler/localrecall:main
    ports:
      - 8080
    environment:
      - COLLECTION_DB_PATH=/db
      - EMBEDDING_MODEL=granite-embedding-107m-multilingual
      - FILE_ASSETS=/assets
      - OPENAI_API_KEY=${LOCAL_AI_API_KEY}
      - OPENAI_BASE_URL=http://localai:8080
    volumes:
      - ./volumes/localrag/db:/db
      - ./volumes/localrag/assets/:/assets

  localrecall-healthcheck:
    depends_on:
      localrecall:
        condition: service_started
    image: busybox
    command: ["sh", "-c", "until wget -q -O - http://localrecall:8080 > /dev/null 2>&1; do echo 'Waiting for localrecall...'; sleep 1; done; echo 'localrecall is up!'"]

  localagi:
    depends_on:
      localai:
        condition: service_healthy
      localrecall-healthcheck:
        condition: service_completed_successfully
      dind:
        condition: service_healthy
    build:
      context: .
      dockerfile: Dockerfile.webui
    ports:
      - 8080:3000
    #image: quay.io/mudler/localagi:master
    environment:
      - LOCALAGI_MODEL=${MODEL_NAME:-gemma-3-4b-it-qat}
      - LOCALAGI_MULTIMODAL_MODEL=${MULTIMODAL_MODEL:-moondream2-20250414}
      - LOCALAGI_IMAGE_MODEL=${IMAGE_MODEL:-sd-1.5-ggml}
      - LOCALAGI_LLM_API_URL=http://localai:8080
      - LOCALAGI_LLM_API_KEY=${LOCAL_AI_API_KEY}
      - LOCALAGI_LOCALRAG_URL=http://localrecall:8080
      - LOCALAGI_STATE_DIR=/pool
      - LOCALAGI_TIMEOUT=5m
      - LOCALAGI_ENABLE_CONVERSATIONS_LOGGING=false
      - LOCALAGI_SSHBOX_URL=root:root@sshbox:22
      - DOCKER_HOST=tcp://dind:2375
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./volumes/localagi/:/pool

volumes:
  anythingllm_storage:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${LOCAL_STORAGE_DIR}
